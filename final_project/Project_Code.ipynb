{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have used the following Versions:\n",
      "('Numpy Version:', '1.16.2')\n",
      "('Pandas Version:', u'0.24.2')\n",
      "('SkLearn Version:', '0.20.3')\n",
      "('Scipy Version:', '1.2.1')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import tester \n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "print(\"I have used the following Versions:\")\n",
    "print(\"Numpy Version:\", np.__version__)\n",
    "print(\"Pandas Version:\", pd.__version__)\n",
    "print(\"SkLearn Version:\", sklearn.__version__)\n",
    "print(\"Scipy Version:\", scipy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion code\n",
    "\n",
    "# content = ''\n",
    "# outsize = 0\n",
    "# with open('final_project_dataset.pkl', 'rb') as infile:\n",
    "#     content = infile.read()\n",
    "# with open('final_project_dataset_new.pkl', 'wb') as output:\n",
    "#     for line in content.splitlines():\n",
    "#         outsize += len(line) + 1\n",
    "#         output.write(line + str.encode('\\n'))\n",
    "\n",
    "# print(\"Done. Saved %s bytes.\" % (len(content)-outsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Select what features you'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    126\n",
      "True      17\n",
      "Name: poi, dtype: int64\n",
      "False    0.863014\n",
      "True     0.116438\n",
      "Name: poi, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi',\n",
    "                'salary',\n",
    "                'bonus', \n",
    "                'long_term_incentive', \n",
    "                'deferred_income', \n",
    "                'deferral_payments',\n",
    "                'loan_advances', \n",
    "                'other',\n",
    "                'expenses', \n",
    "                'director_fees',\n",
    "                'total_payments',\n",
    "                'exercised_stock_options',\n",
    "                'restricted_stock',\n",
    "                'restricted_stock_deferred',\n",
    "                'total_stock_value',\n",
    "                'to_messages',\n",
    "                'from_messages',\n",
    "                'from_this_person_to_poi',\n",
    "                'from_poi_to_this_person']\n",
    "### Load the dictionary containing the dataset\n",
    "# with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "#     data_dict = pickle.load(data_file)\n",
    "# data_dict\n",
    "\n",
    "with open('final_project_dataset_new.pkl', 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "name_keys = sorted(list(data_dict.keys()))\n",
    "rows = len(name_keys)\n",
    "class_counts = data_frame[\"poi\"].value_counts()\n",
    "class_priors = class_counts / rows\n",
    "print(class_counts)\n",
    "print(class_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 19 columns):\n",
      "poi                          146 non-null bool\n",
      "salary                       95 non-null float64\n",
      "bonus                        82 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "deferral_payments            39 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "other                        93 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "exercised_stock_options      102 non-null float64\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "dtypes: bool(1), float64(18)\n",
      "memory usage: 21.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Converting Dictionary to Numpy Array\n",
    "data_frame = pd.DataFrame.from_dict(data_dict, orient = 'index')\n",
    "#Order columns in DataFrame, exclude email column\n",
    "data_frame = data_frame[features_list]\n",
    "data_frame = data_frame.replace('NaN', np.nan)\n",
    "data_frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI / non-POI split\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "non-POI    128\n",
       "POI         18\n",
       "Name: poi, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split of POI and non-POI in the dataset\n",
    "poi_non_poi = data_frame.poi.value_counts()\n",
    "poi_non_poi.index=['non-POI', 'POI']\n",
    "print \"POI / non-POI split\"\n",
    "poi_non_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unrequired columns\n",
    "# poi_names = data_frame.pop('poi_name')\n",
    "# poi_labels = data_frame.pop('poi')\n",
    "# emails = data_frame.pop('email_address')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poi                            0\n",
      "salary                        51\n",
      "bonus                         64\n",
      "long_term_incentive           80\n",
      "deferred_income               97\n",
      "deferral_payments            107\n",
      "loan_advances                142\n",
      "other                         53\n",
      "expenses                      51\n",
      "director_fees                129\n",
      "total_payments                21\n",
      "exercised_stock_options       44\n",
      "restricted_stock              36\n",
      "restricted_stock_deferred    128\n",
      "total_stock_value             20\n",
      "to_messages                   60\n",
      "from_messages                 60\n",
      "from_this_person_to_poi       60\n",
      "from_poi_to_this_person       60\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Counting NaN Values for each Column\n",
    "nan_vals = data_frame.isnull().sum(axis = 0)\n",
    "print(nan_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Replacing 'NaN' in financial features with 0\n",
    "data_frame.iloc[:,:15] = data_frame.iloc[:,:15].fillna(0)\n",
    "\n",
    "email_features = ['to_messages', 'from_messages', 'from_this_person_to_poi', 'from_poi_to_this_person']\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "\n",
    "#impute missing values of email features \n",
    "data_frame.loc[data_frame[data_frame.poi == 1].index,email_features] = imp.fit_transform(data_frame[email_features][data_frame.poi == 1])\n",
    "data_frame.loc[data_frame[data_frame.poi == 0].index,email_features] = imp.fit_transform(data_frame[email_features][data_frame.poi == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = data_frame.quantile(.5) + 1.5 * (data_frame.quantile(.75)-data_frame.quantile(.25))\n",
    "pd.DataFrame((data_frame[1:] > outliers[1:]).sum(axis = 1), columns = ['# of outliers']).\\\n",
    "    sort_values('# of outliers',  ascending = [0]).head(7)\n",
    "\n",
    "# exclude 3 outliers from the data set\n",
    "data_frame = data_frame.drop(['TOTAL', 'SKILLING JEFFREY K', 'FREVERT MARK A'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "#Create new feature(s)\n",
    "data_frame[\"fraction_from_poi\"] = data_frame[\"from_poi_to_this_person\"].\\\n",
    "divide(data_frame[\"to_messages\"], fill_value = 0)\n",
    "\n",
    "data_frame[\"fraction_to_poi\"] = data_frame[\"from_this_person_to_poi\"].\\\n",
    "divide(data_frame[\"from_messages\"], fill_value = 0)\n",
    "\n",
    "data_frame[\"fraction_from_poi\"] = data_frame[\"fraction_from_poi\"].fillna(0.0)\n",
    "data_frame[\"fraction_to_poi\"] = data_frame[\"fraction_to_poi\"].fillna(0.0)\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_frame.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poi                          0\n",
      "salary                       0\n",
      "bonus                        0\n",
      "long_term_incentive          0\n",
      "deferred_income              0\n",
      "deferral_payments            0\n",
      "loan_advances                0\n",
      "other                        0\n",
      "expenses                     0\n",
      "director_fees                0\n",
      "total_payments               0\n",
      "exercised_stock_options      0\n",
      "restricted_stock             0\n",
      "restricted_stock_deferred    0\n",
      "total_stock_value            0\n",
      "to_messages                  0\n",
      "from_messages                0\n",
      "from_this_person_to_poi      0\n",
      "from_poi_to_this_person      0\n",
      "fraction_from_poi            0\n",
      "fraction_to_poi              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_vals = data_frame.isnull().sum(axis = 0)\n",
    "print(nan_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fraction_to_poi', 0.3474206349206347]\n",
      "['expenses', 0.32768459235546066]\n",
      "['to_messages', 0.16170357031701577]\n",
      "['total_stock_value', 0.08901338313103019]\n",
      "['deferred_income', 0.07417781927585848]\n"
     ]
    }
   ],
   "source": [
    "#Decision tree using features with non-null importance\n",
    "clf = DecisionTreeClassifier(random_state = 75)\n",
    "clf.fit(data_frame.iloc[:,1:], data_frame.iloc[:,:1])\n",
    "\n",
    "# show the features with non null importance, sorted and create features_list of features for the model\n",
    "features_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0:\n",
    "        features_importance.append([data_frame.columns[i+1], clf.feature_importances_[i]])\n",
    "features_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for f_i in features_importance:\n",
    "    print f_i\n",
    "features_list = [x[0] for x in features_importance]\n",
    "features_list.insert(0, 'poi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_frame, labels, random_state = 100)\n",
    "clf = GaussianNB()\n",
    "clf.fit(x_train, y_train)\n",
    "poi_labels = data_frame.pop('poi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "('Gaussian Naive Bayes Accuracy:', 0.8888888888888888)\n",
      "('Gaussian Naive Bayes CV Score:', 0.7314614121510674)\n",
      "('Gaussian Naive Bayes Precision:', 0.8380952380952381)\n",
      "('Gaussian Naive Bayes Recall:', 0.8888888888888888)\n",
      "('Gaussian Naive Bayes F1-Score:', 0.8627450980392157)\n",
      "---------------------------------------------------\n",
      "('KNN Accuracy:', 0.9166666666666666)\n",
      "('KNN CV Score:', 0.8743185550082101)\n",
      "('KNN Precision:', 0.8402777777777778)\n",
      "('KNN Recall:', 0.9166666666666666)\n",
      "('KNN F1-Score:', 0.8768115942028986)\n",
      "---------------------------------------------------\n",
      "('Logistic Regression Accuracy:', 0.7777777777777778)\n",
      "('Logistic Regression CV Score:', 0.8324137931034483)\n",
      "('Logistic Regression Precision:', 0.8279569892473118)\n",
      "('Logistic Regression Recall:', 0.7777777777777778)\n",
      "('Logistic Regression F1-Score:', 0.8020833333333334)\n",
      "---------------------------------------------------\n",
      "('Decision Tree Accuracy:', 1.0)\n",
      "('Decision Tree CV Score:', 0.8883743842364533)\n",
      "('Decision Tree Precision:', 1.0)\n",
      "('Decision Tree Recall:', 1.0)\n",
      "('Decision Tree F1-Score:', 1.0)\n",
      "---------------------------------------------------\n",
      "('Random Forrest Accuracy:', 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Random Forrest CV Score:', 0.9164532019704434)\n",
      "('Random Forrest Precision:', 1.0)\n",
      "('Random Forrest Recall:', 1.0)\n",
      "('Random Forrest F1-Score:', 1.0)\n",
      "---------------------------------------------------\n",
      "('AdaBoost Accuracy:', 1.0)\n",
      "('AdaBoost CV Score:', 0.9086042692939245)\n",
      "('AdaBoost Precision:', 1.0)\n",
      "('AdaBoost Recall:', 1.0)\n",
      "('AdaBoost F1-Score:', 1.0)\n",
      "---------------------------------------------------\n",
      "('Gradient Boosting Accuracy:', 1.0)\n",
      "('Gradient Boosting CV Score:', 0.902183908045977)\n",
      "('Gradient Boosting Precision:', 1.0)\n",
      "('Gradient Boosting Recall:', 1.0)\n",
      "('Gradient Boosting F1-Score:', 1.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = clf.predict(x_test)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Gaussian Naive Bayes Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Gaussian Naive Bayes CV Score:\", cross_val_score(clf, data_frame, poi_labels, cv=5).mean())\n",
    "print(\"Gaussian Naive Bayes Precision:\", precision_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"Gaussian Naive Bayes Recall:\", recall_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"Gaussian Naive Bayes F1-Score:\", f1_score(y_test, preds, average=\"weighted\"))\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=7)\n",
    "clf.fit(x_train, y_train)\n",
    "preds = clf.predict(x_test)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"KNN Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"KNN CV Score:\", cross_val_score(clf, data_frame, poi_labels, cv=5).mean())\n",
    "print(\"KNN Precision:\", precision_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"KNN Recall:\", recall_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"KNN F1-Score:\", f1_score(y_test, preds, average=\"weighted\"))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)\n",
    "preds = clf.predict(x_test)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Logistic Regression CV Score:\", cross_val_score(clf, data_frame, poi_labels, cv=5).mean())\n",
    "print(\"Logistic Regression Precision:\", precision_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"Logistic Regression Recall:\", recall_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"Logistic Regression F1-Score:\", f1_score(y_test, preds, average=\"weighted\"))\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth=5, min_samples_leaf=5)\n",
    "clf.fit(x_train, y_train)\n",
    "preds = clf.predict(x_test)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Decision Tree CV Score:\", cross_val_score(clf, data_frame, poi_labels, cv=5).mean())\n",
    "print(\"Decision Tree Precision:\", precision_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"Decision Tree Recall:\", recall_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"Decision Tree F1-Score:\", f1_score(y_test, preds, average=\"weighted\"))\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 20, criterion = \"entropy\", random_state = 100, max_depth=3, min_samples_leaf=5)\n",
    "clf.fit(x_train, y_train)\n",
    "preds = clf.predict(x_test)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Random Forrest Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Random Forrest CV Score:\", cross_val_score(clf, data_frame, poi_labels, cv=5).mean())\n",
    "print(\"Random Forrest Precision:\", precision_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"Random Forrest Recall:\", recall_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"Random Forrest F1-Score:\", f1_score(y_test, preds, average=\"weighted\"))\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators = 20, random_state = 100)\n",
    "clf.fit(x_train, y_train)\n",
    "preds = clf.predict(x_test)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"AdaBoost CV Score:\", cross_val_score(clf, data_frame, poi_labels, cv=5).mean())\n",
    "print(\"AdaBoost Precision:\", precision_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"AdaBoost Recall:\", recall_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"AdaBoost F1-Score:\", f1_score(y_test, preds, average=\"weighted\"))\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators = 20)\n",
    "clf.fit(x_train, y_train)\n",
    "preds = clf.predict(x_test)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Gradient Boosting CV Score:\", cross_val_score(clf, data_frame, poi_labels, cv=5).mean())\n",
    "print(\"Gradient Boosting Precision:\", precision_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"Gradient Boosting Recall:\", recall_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"Gradient Boosting F1-Score:\", f1_score(y_test, preds, average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Maximum Score =', 0.9874351395730706, ' with n_estimators =', 11, 'and learning rate =', 0.7)\n",
      "('AdaBoost Accuracy:', 1.0)\n",
      "('AdaBoost CV Score:', 0.9371756978653532)\n",
      "('AdaBoost Precision:', 1.0)\n",
      "('AdaBoost Recall:', 1.0)\n",
      "('AdaBoost F1-Score:', 1.0)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Source:https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234\n",
    "Classification Accuracy is what we usually mean, when we use the term accuracy.\n",
    "It is the ratio of number of correct predictions to the total number of input samples.\n",
    "\n",
    "Precision : It is the number of correct positive results divided by the number of positive results\n",
    "predicted by the classifier.\n",
    "\n",
    "Recall : It is the number of correct positive results divided by the number of all relevant samples \n",
    "(all samples that should have been identified as positive).\n",
    "\n",
    "'''\n",
    "# I am choosing the AdaBoost Classifier\n",
    "\n",
    "# Fine Tuning\n",
    "num_estimators = range(1, 31, 2)\n",
    "learning_rates = list(np.linspace(0.1, 2, 20, dtype=np.float32))\n",
    "\n",
    "scores = np.zeros((len(num_estimators), len(learning_rates)))\n",
    "\n",
    "for ni, ne in enumerate(num_estimators):\n",
    "    for li, lr in enumerate(learning_rates):\n",
    "        # Classifier with the Changing Parameters\n",
    "        clf = AdaBoostClassifier(n_estimators = ne, learning_rate = lr, random_state = 100)\n",
    "        clf.fit(x_train, y_train)\n",
    "        preds = clf.predict(x_test)\n",
    "        \n",
    "        # Computing Cusomized Score\n",
    "        score = accuracy_score(y_test, preds) + \\\n",
    "                cross_val_score(clf, data_frame, poi_labels, cv=5).mean() + \\\n",
    "                precision_score(y_test, preds, average=\"weighted\") + \\\n",
    "                recall_score(y_test, preds, average=\"weighted\") + \\\n",
    "                f1_score(y_test, preds, average=\"weighted\")\n",
    "        scores[ni][li] = score / 5.0\n",
    "\n",
    "max_score = np.max(scores)\n",
    "max_idxs = np.where(scores == max_score)\n",
    "best_ne = num_estimators[max_idxs[0][0]]\n",
    "best_lr = learning_rates[max_idxs[1][0]]\n",
    "print(\"Maximum Score =\", max_score, \" with n_estimators =\", best_ne, \"and learning rate =\", best_lr)\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators = best_ne, learning_rate=best_lr, random_state = 100)\n",
    "clf.fit(x_train, y_train)\n",
    "preds = clf.predict(x_test)\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"AdaBoost CV Score:\", cross_val_score(clf, data_frame, poi_labels, cv=5).mean())\n",
    "print(\"AdaBoost Precision:\", precision_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"AdaBoost Recall:\", recall_score(y_test, preds, average=\"weighted\"))\n",
    "print(\"AdaBoost F1-Score:\", f1_score(y_test, preds, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
